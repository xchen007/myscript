
#!/usr/bin/env python3
"""
ä¼˜åŒ–ç‰ˆ sync_local_to_pod è„šæœ¬
- æ”¯æŒé€šè¿‡ pod label è‡ªåŠ¨é€‰æ‹© running pod
- æ”¯æŒ --compress-thresholdï¼ˆé»˜è®¤50ï¼Œé…ç½®æ–‡ä»¶æŒä¹…åŒ–ï¼‰
- æ”¯æŒ --force-full-sync å¼ºåˆ¶å…¨é‡åŒæ­¥
- é…ç½®æ–‡ä»¶å­˜å‚¨äº ~/.sync2pod/$project_name/.sync_config.json
- MD5 å¯¹æ¯”å¢é‡åŒæ­¥
- å®æ—¶æ–‡ä»¶ç›‘å¬ï¼ˆwatchdogï¼‰
- å¤šçº¿ç¨‹å¹¶å‘ä¸Šä¼ 
- è¿›åº¦ä¸è€—æ—¶å±•ç¤º
"""
import argparse
import os
import sys
import json
import tarfile
import tempfile
import shutil
import hashlib
import time
import subprocess
import threading
from pathlib import Path
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
from concurrent.futures import ThreadPoolExecutor
from loguru import logger

# ========== æ—¥å¿—é…ç½® ==========

def configure_logger(debug=False):
    """é…ç½® loguru æ—¥å¿—"""
    # ç§»é™¤é»˜è®¤çš„ handler
    logger.remove()
    
    # æ·»åŠ æ§åˆ¶å°è¾“å‡º
    if debug:
        # Debug æ¨¡å¼ï¼šæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…å«DEBUGçº§åˆ«æ—¥å¿—ï¼‰
        logger.add(
            sys.stderr,
            format="<green>{time:HH:mm:ss.SSS}</green> | <level>{level: <8}</level> | <level>{message}</level>",
            level="DEBUG"
        )
    else:
        # æ™®é€šæ¨¡å¼ï¼šæ˜¾ç¤ºæ—¶é—´ã€çº§åˆ«å’Œæ¶ˆæ¯ï¼ˆINFOåŠä»¥ä¸Šï¼‰
        logger.add(
            sys.stderr,
            format="<green>{time:HH:mm:ss.SSS}</green> | <level>{level: <8}</level> | <level>{message}</level>",
            level="INFO"
        )
    
    # æ·»åŠ æ–‡ä»¶æ—¥å¿—ï¼ˆå¯é€‰ï¼‰
    # logger.add("sync2pod_{time}.log", rotation="10 MB", retention="7 days", level="DEBUG")

# ========== é…ç½®ç®¡ç† ==========

def get_config_path(project_name):
    home = Path.home()
    config_dir = home / '.sync2pod' / project_name
    config_dir.mkdir(parents=True, exist_ok=True)
    return config_dir / '.sync_config.json'

def load_config(project_name):
    config_path = get_config_path(project_name)
    if config_path.exists():
        with open(config_path, 'r') as f:
            config = json.load(f)
    else:
        config = {}
    # é»˜è®¤å­—æ®µ
    config.setdefault('compress_threshold', 50)
    return config

def save_config(project_name, config):
    config_path = get_config_path(project_name)
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=4)

# ========== K8s Pod é€‰æ‹©é€»è¾‘ ==========

def select_running_pod_by_label(cluster, namespace, pod_label):
    """
    é€šè¿‡ label é€‰æ‹© running çŠ¶æ€çš„ podï¼Œè¿”å› pod name
    éœ€ä¾èµ– kubectl
    """
    import subprocess
    label_selector = pod_label
    TESS_KUBECTL = ['tess', 'kubectl']
    jsonpath = "{.items[0].metadata.name}"
    cmd = TESS_KUBECTL.copy()
    if cluster:
        cmd += ['--cluster', str(cluster)]
    cmd += [
        'get', 'pods',
        '-n', namespace,
        '-l', label_selector,
        '--field-selector=status.phase=Running',
        '-o', f"jsonpath='{jsonpath}'"
    ]
    # debugè¾“å‡ºæ—¶ä¸ºshellå‹å¥½åŠ å•å¼•å·ï¼ˆå»é‡ -oï¼‰
    cmd_str = ' '.join(cmd)
    # æ£€æŸ¥ debug ç¯å¢ƒå˜é‡
    debug = os.environ.get('SYNC2POD_DEBUG', '').lower() == 'true'
    if not debug:
        # å…¼å®¹ä¸»æµç¨‹ä¼ é€’ debug
        import inspect
        frame = inspect.currentframe().f_back
        debug = frame.f_locals.get('debug', False)
    if debug:
        logger.debug(f'æŸ¥è¯¢ running pod å‘½ä»¤: {cmd_str}')
    try:
        pod_name = subprocess.check_output(cmd, text=True).strip()
        # å»é™¤é¦–å°¾å•å¼•å·å’Œç©ºç™½
        pod_name = pod_name.strip("'\"").strip()
        if not pod_name:
            logger.error('æœªæ‰¾åˆ° running çŠ¶æ€çš„ pod')
            sys.exit(1)
        return pod_name
    except Exception as e:
        logger.error(f'kubectl æŸ¥è¯¢ pod å¤±è´¥: {cmd_str}\n{e}')
        sys.exit(1)

# ========== å·¥å…·å‡½æ•° ==========



def calculate_file_md5(file_path):
    """è®¡ç®—æ–‡ä»¶çš„ MD5 å“ˆå¸Œå€¼"""
    hash_md5 = hashlib.md5()
    try:
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except Exception as e:
        logger.error(f"è®¡ç®— MD5 å¤±è´¥ {file_path}: {e}")
        return None

def format_file_size(size_bytes):
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°æ˜¾ç¤º"""
    if size_bytes == 0:
        return "0 B"
    
    size_names = ["B", "KB", "MB", "GB", "TB"]
    import math
    i = int(math.floor(math.log(size_bytes, 1024)))
    p = math.pow(1024, i)
    s = round(size_bytes / p, 2)
    return f"{s} {size_names[i]}"

# ========== æ–‡ä»¶åŒæ­¥é€»è¾‘ ==========

def count_files(local_path, exclude_paths=None):
    """ç»Ÿè®¡éœ€è¦åŒæ­¥çš„æ–‡ä»¶æ•°é‡ï¼ˆæ’é™¤éšè—æ–‡ä»¶ï¼‰"""
    cnt = 0
    for root, dirs, files in os.walk(local_path):
        # æ’é™¤éšè—ç›®å½•
        dirs[:] = [d for d in dirs if not d.startswith('.')]
        
        for file in files:
            # è·³è¿‡éšè—æ–‡ä»¶
            if file.startswith('.'):
                continue
            
            file_path = os.path.join(root, file)
            rel_path = os.path.relpath(file_path, local_path)
            
            # æ£€æŸ¥æ˜¯å¦åœ¨æ’é™¤è·¯å¾„ä¸­
            should_exclude = False
            if exclude_paths:
                for exclude_path in exclude_paths:
                    if rel_path == exclude_path or rel_path.startswith(exclude_path + os.sep) or rel_path.startswith(exclude_path + '/'):
                        should_exclude = True
                        break
            
            if not should_exclude:
                cnt += 1
    
    return cnt

def compress_dir(src_dir, out_file, exclude_paths=None, debug=False):
    """å‹ç¼©ç›®å½•ä¸º tar.gz æ–‡ä»¶ï¼ˆæ’é™¤éšè—æ–‡ä»¶å’Œ exclude_paths ä¸­çš„ç›®å½•ï¼‰"""
    if exclude_paths is None:
        exclude_paths = []
    
    # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰å¾…å‹ç¼©çš„æ–‡ä»¶ä¿¡æ¯
    files_to_compress = []  # å­˜å‚¨ (file_path, rel_path, file_size)
    
    logger.info("ğŸ” æ‰«æå¾…å‹ç¼©æ–‡ä»¶...")
    for root, dirs, files in os.walk(src_dir):
        # æ’é™¤éšè—ç›®å½•
        dirs[:] = [d for d in dirs if not d.startswith('.')]
        
        # æ’é™¤ exclude_paths ä¸­çš„ç›®å½•ï¼ˆç›´æ¥ä¿®æ”¹ dirs åˆ—è¡¨ï¼Œé¿å…éå†è¿™äº›ç›®å½•ï¼‰
        dirs_to_remove = []
        for d in dirs:
            dir_path = os.path.join(root, d)
            rel_dir_path = os.path.relpath(dir_path, src_dir)
            
            for exclude_path in exclude_paths:
                # æ£€æŸ¥ç›®å½•æ˜¯å¦åŒ¹é…æ’é™¤æ¨¡å¼
                if rel_dir_path == exclude_path or rel_dir_path.startswith(exclude_path + os.sep) or rel_dir_path.startswith(exclude_path + '/'):
                    dirs_to_remove.append(d)
                    if debug:
                        logger.debug(f'æ’é™¤ç›®å½•: {rel_dir_path}')
                    break
        
        # ä» dirs åˆ—è¡¨ä¸­ç§»é™¤éœ€è¦æ’é™¤çš„ç›®å½•ï¼Œos.walk å°†ä¸ä¼šéå†è¿™äº›ç›®å½•
        for d in dirs_to_remove:
            dirs.remove(d)
        
        for file in files:
            # è·³è¿‡éšè—æ–‡ä»¶
            if file.startswith('.'):
                continue
            
            file_path = os.path.join(root, file)
            rel_path = os.path.relpath(file_path, src_dir)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åº”æ’é™¤
            should_exclude = False
            for exclude_path in exclude_paths:
                if rel_path == exclude_path or rel_path.startswith(exclude_path + os.sep) or rel_path.startswith(exclude_path + '/'):
                    should_exclude = True
                    if debug:
                        logger.debug(f'æ’é™¤æ–‡ä»¶: {rel_path}')
                    break
            
            if not should_exclude:
                try:
                    file_size = os.path.getsize(file_path)
                    files_to_compress.append((file_path, rel_path, file_size))
                except Exception as e:
                    if debug:
                        logger.debug(f'è·å–æ–‡ä»¶å¤§å°å¤±è´¥ {rel_path}: {e}')
    
    # ç¬¬äºŒæ­¥ï¼šæ˜¾ç¤ºæœ€å¤§çš„10ä¸ªæ–‡ä»¶
    if files_to_compress:
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š ä½“ç§¯æœ€å¤§çš„ 10 ä¸ªæ–‡ä»¶:")
        logger.info("=" * 60)
        sorted_files = sorted(files_to_compress, key=lambda x: x[2], reverse=True)[:10]
        for idx, (_, rel_path, size) in enumerate(sorted_files, 1):
            size_str = format_file_size(size)
            logger.info(f"  {idx:2d}. {size_str:>10s}  {rel_path}")
        logger.info("=" * 60 + "\n")
    
    # ç¬¬ä¸‰æ­¥ï¼šå‹ç¼©æ–‡ä»¶
    logger.info("ğŸ“¦ å¼€å§‹å‹ç¼©æ–‡ä»¶...")
    added_count = 0
    with tarfile.open(out_file, 'w:gz') as tar:
        for file_path, rel_path, _ in files_to_compress:
            tar.add(file_path, arcname=rel_path)
            added_count += 1
    
    logger.success(f'âœ… å‹ç¼©å®Œæˆ: å·²æ‰“åŒ… {added_count} ä¸ªæ–‡ä»¶')

def get_remote_files_md5(pod_name, namespace, cluster, remote_path, debug=False, exclude_paths=None):
    """è·å– Pod ä¸­æ‰€æœ‰æ–‡ä»¶çš„ MD5 å€¼ï¼ˆæ’é™¤ exclude_pathsï¼‰"""
    if exclude_paths is None:
        exclude_paths = []
    
    remote_files = {}
    try:
        # è·å–è¿œç¨‹ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        command = f'tess kubectl --cluster {cluster} -n {namespace} exec {pod_name} -- find {remote_path} -type f -exec md5sum {{}} \\; 2>/dev/null || echo ""'
        if debug:
            logger.debug(f'æ‰§è¡Œå‘½ä»¤: {command}')
        result = subprocess.run(command, shell=True, capture_output=True, text=True)
        
        if result.returncode == 0 and result.stdout.strip():
            for line in result.stdout.strip().split('\n'):
                if line.strip():
                    parts = line.split()
                    if len(parts) >= 2:
                        md5_value = parts[0]
                        file_path = ' '.join(parts[1:])
                        # è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„
                        if file_path.startswith(remote_path):
                            rel_path = file_path[len(remote_path):].lstrip('/')
                            
                            # æ£€æŸ¥æ˜¯å¦åº”æ’é™¤
                            should_exclude = False
                            for exclude_path in exclude_paths:
                                if rel_path == exclude_path or rel_path.startswith(exclude_path + os.sep) or rel_path.startswith(exclude_path + '/'):
                                    should_exclude = True
                                    if debug:
                                        logger.debug(f'æ’é™¤è¿œç¨‹æ–‡ä»¶: {rel_path} (åŒ¹é…æ’é™¤æ¨¡å¼: {exclude_path})')
                                    break
                            
                            if not should_exclude:
                                remote_files[rel_path] = md5_value
                                if debug:
                                    logger.debug(f'è¿œç¨‹æ–‡ä»¶: {rel_path} -> {md5_value}')
    except Exception as e:
        logger.error(f"è·å–è¿œç¨‹æ–‡ä»¶ MD5 å¤±è´¥: {e}")
    
    return remote_files

def upload_initial_files(local_path, namespace, pod_name, remote_path, cluster, debug=False, max_workers=10, exclude_paths=None):
    """åˆå§‹ä¸Šä¼ ï¼šMD5 å¯¹æ¯”åä»…ä¸Šä¼ æœ‰å˜åŒ–çš„æ–‡ä»¶"""
    start_time = time.time()
    
    if exclude_paths is None:
        exclude_paths = []
    
    # ç¡®ä¿è¿œç¨‹ç›®å½•å­˜åœ¨
    logger.info("ğŸ” æ£€æŸ¥è¿œç¨‹ç›®å½•å¹¶æ”¶é›†è¿œç¨‹æ¸…å•...")
    ensure_dir_cmd = f'tess kubectl --cluster {cluster} -n {namespace} exec {pod_name} -- mkdir -p {remote_path}'
    if debug:
        logger.debug(f'æ‰§è¡Œå‘½ä»¤: {ensure_dir_cmd}')
    try:
        result = subprocess.run(ensure_dir_cmd, shell=True, capture_output=True, text=True, check=True)
    except subprocess.CalledProcessError as e:
        if e.stderr and e.stderr.strip():
            logger.error(f"âŒ å‘½ä»¤æ‰§è¡Œé”™è¯¯: {e.stderr.strip()}")
        logger.error(f"âŒ ç¡®ä¿è¿œç¨‹ç›®å½•å¤±è´¥: é”™è¯¯ç  {e.returncode}")
        return
    
    # è·å–è¿œç¨‹æ–‡ä»¶ MD5
    logger.info("è·å–è¿œç¨‹æ–‡ä»¶ MD5 å€¼...")
    remote_files_md5 = get_remote_files_md5(pod_name, namespace, cluster, remote_path, debug, exclude_paths)
    logger.info(f"è¿œç¨‹æ–‡ä»¶æ•°é‡: {len(remote_files_md5)}")
    
    # æ”¶é›†éœ€è¦ä¸Šä¼ çš„æ–‡ä»¶å’Œéœ€è¦åˆ›å»ºçš„ç›®å½•
    directories_to_create = set()
    files_to_upload = []
    files_skipped = 0
    
    for root, dirs, files in os.walk(local_path):
        # æ’é™¤éšè—ç›®å½•
        dirs[:] = [d for d in dirs if not d.startswith('.')]
        
        for file in files:
            # è·³è¿‡éšè—æ–‡ä»¶
            if file.startswith('.'):
                continue
            
            file_path = os.path.join(root, file)
            rel_path = os.path.relpath(file_path, local_path)
            
            # æ£€æŸ¥æ˜¯å¦åº”æ’é™¤
            should_exclude = False
            for exclude_path in exclude_paths:
                if rel_path == exclude_path or rel_path.startswith(exclude_path + os.sep) or rel_path.startswith(exclude_path + '/'):
                    should_exclude = True
                    if debug:
                        logger.debug(f'æ’é™¤æ–‡ä»¶: {rel_path} (åŒ¹é…æ’é™¤æ¨¡å¼: {exclude_path})')
                    break
            
            if should_exclude:
                continue
            
            # è®¡ç®—æœ¬åœ°æ–‡ä»¶ MD5
            local_md5 = calculate_file_md5(file_path)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸Šä¼ 
            need_upload = True
            if rel_path in remote_files_md5:
                remote_md5 = remote_files_md5[rel_path]
                if local_md5 == remote_md5:
                    need_upload = False
                    files_skipped += 1
            
            if need_upload:
                remote_dir = os.path.dirname(os.path.join(remote_path, rel_path))
                directories_to_create.add(remote_dir)
                files_to_upload.append((file_path, rel_path))
    
    local_total_files = files_skipped + len(files_to_upload)
    logger.info(f"ğŸ“Š æ¸…å•æ±‡æ€» -> è¿œç¨‹: {len(remote_files_md5)} | æœ¬åœ°: {local_total_files} | å¾…ä¸Šä¼ : {len(files_to_upload)}")
    
    # æ™ºèƒ½å¿«é€Ÿè·¯å¾„ï¼šå¦‚æœå¾…ä¸Šä¼ æ–‡ä»¶æ•°è¶…è¿‡100ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°å‹ç¼©æ‰“åŒ…ä¸Šä¼ 
    if len(files_to_upload) > 100:
        logger.info("ğŸ—œï¸  å¾…ä¸Šä¼ æ–‡ä»¶æ•°è¶…è¿‡é˜ˆå€¼ (100)ï¼Œä½¿ç”¨å‹ç¼©æ‰“åŒ…å¿«é€Ÿè·¯å¾„...")
        try:
            # åˆ›å»ºå‹ç¼©åŒ…
            logger.info("ğŸ“¦ æ­£åœ¨åˆ›å»ºå‹ç¼©åŒ…...")
            compress_start = time.time()
            with tempfile.TemporaryDirectory() as tmpdir:
                tar_path = os.path.join(tmpdir, 'sync_upload.tar.gz')
                compress_dir(local_path, tar_path, exclude_paths, debug)
                compress_end = time.time()
                compress_time = compress_end - compress_start
                
                compressed_size = os.path.getsize(tar_path)
                logger.success(f"âœ… å‹ç¼©å®Œæˆ: {format_file_size(compressed_size)} (è€—æ—¶: {compress_time:.2f}s)")
                
                # ä¸Šä¼ åˆ°ä¸´æ—¶ä½ç½®
                remote_tmp_tar = "/tmp/sync_archive.tar.gz"
                upload_cmd = f'tess kubectl --cluster {cluster} -n {namespace} cp {tar_path} {pod_name}:{remote_tmp_tar}'
                if debug:
                    logger.debug(f'æ‰§è¡Œå‘½ä»¤: {upload_cmd}')
                logger.info("ğŸ“¤ ä¸Šä¼ å‹ç¼©åŒ…åˆ° Pod /tmp...")
                upload_start = time.time()
                result = subprocess.run(upload_cmd, shell=True, capture_output=True, text=True, check=True)
                upload_end = time.time()
                upload_time = upload_end - upload_start
                logger.success(f"âœ… å‹ç¼©åŒ…ä¸Šä¼ æˆåŠŸ (è€—æ—¶: {upload_time:.2f}s)")
                
                # è§£å‹åˆ°ç›®æ ‡ç›®å½•ï¼ˆè¦†ç›–æ¨¡å¼ï¼‰
                extract_cmd = f'tess kubectl --cluster {cluster} -n {namespace} exec {pod_name} -- bash -c "mkdir -p {remote_path} && tar -xzf {remote_tmp_tar} -C {remote_path} && rm -f {remote_tmp_tar}"'
                if debug:
                    logger.debug(f'æ‰§è¡Œå‘½ä»¤: {extract_cmd}')
                logger.info("ğŸ“¦ è§£å‹åˆ°è¿œç¨‹è·¯å¾„ï¼ˆè¦†ç›–æ¨¡å¼ï¼‰...")
                extract_start = time.time()
                result = subprocess.run(extract_cmd, shell=True, capture_output=True, text=True, check=True)
                extract_end = time.time()
                extract_time = extract_end - extract_start
                logger.success(f"âœ… è§£å‹å®Œæˆ (è€—æ—¶: {extract_time:.2f}s)")
                
            end_time = time.time()
            total_time = end_time - start_time
            
            logger.info("\n" + "=" * 60)
            logger.info("â±ï¸  å¿«é€Ÿå‹ç¼©åŒæ­¥è€—æ—¶ç»Ÿè®¡")
            logger.info("=" * 60)
            logger.info(f"  1. å‹ç¼©æ–‡ä»¶:   {compress_time:.2f}s")
            logger.info(f"  2. ä¸Šä¼ æ–‡ä»¶:   {upload_time:.2f}s")
            logger.info(f"  3. è¿œç«¯è§£å‹:   {extract_time:.2f}s")
            logger.info(f"  æ€»è€—æ—¶:        {total_time:.2f}s")
            logger.info("=" * 60)
            logger.success("ğŸ‰ åˆå§‹åŒæ­¥å®Œæˆï¼å¼€å§‹æ–‡ä»¶å˜æ›´ç›‘å¬...")
            logger.info("=" * 60)
            return
        except subprocess.CalledProcessError as e:
            if e.stderr and e.stderr.strip():
                logger.error(f"âŒ å‘½ä»¤æ‰§è¡Œé”™è¯¯: {e.stderr.strip()}")
            logger.error(f"âŒ å‹ç¼©å¿«é€Ÿè·¯å¾„å¤±è´¥ (é”™è¯¯ç : {e.returncode})ï¼Œå›é€€åˆ°å¢é‡ä¸Šä¼ ")
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºæˆ–ä¸Šä¼ å‹ç¼©åŒ…å¤±è´¥: {e}ï¼Œå›é€€åˆ°å¢é‡ä¸Šä¼ ")
    
    logger.info("=" * 60)
    logger.info("å¼€å§‹å¢é‡ä¸Šä¼ ...")
    logger.info("=" * 60)
    
    # æ‰¹é‡åˆ›å»ºç›®å½•
    if directories_to_create:
        sorted_dirs = sorted(directories_to_create)
        filtered_dirs = []
        
        for dir_path in sorted_dirs:
            is_redundant = False
            for other_dir in sorted_dirs:
                if other_dir != dir_path and other_dir.startswith(dir_path + '/'):
                    is_redundant = True
                    break
            
            if not is_redundant:
                filtered_dirs.append(dir_path)
        
        if filtered_dirs:
            logger.info(f"\nğŸ“ åˆ›å»ºè¿œç¨‹ç›®å½•: {len(filtered_dirs)} ä¸ªç›®å½•")
            all_dirs = ' '.join(filtered_dirs)
            mkdir_command = f'tess kubectl --cluster {cluster} -n {namespace} exec {pod_name} -- mkdir -p {all_dirs}'
            if debug:
                logger.debug(f'æ‰§è¡Œå‘½ä»¤: {mkdir_command}')
            try:
                result = subprocess.run(mkdir_command, shell=True, capture_output=True, text=True, check=True)
                logger.success(f"âœ… ç›®å½•åˆ›å»ºæˆåŠŸ: {len(filtered_dirs)} ä¸ªç›®å½•")
            except subprocess.CalledProcessError as e:
                if e.stderr and e.stderr.strip():
                    logger.error(f"âŒ å‘½ä»¤æ‰§è¡Œé”™è¯¯: {e.stderr.strip()}")
                logger.error(f"âŒ ç›®å½•åˆ›å»ºå¤±è´¥: é”™è¯¯ç  {e.returncode}")
    
    # å¹¶å‘ä¸Šä¼ æ–‡ä»¶
    if files_to_upload:
        logger.info(f"\nå¼€å§‹å¹¶å‘æ–‡ä»¶ä¸Šä¼ ... (æœ€å¤§å¹¶å‘æ•°: {max_workers})")
        
        completed_files = 0
        total_files = len(files_to_upload)
        
        def upload_single_file(file_info):
            nonlocal completed_files
            file_path, rel_path = file_info
            file_size = os.path.getsize(file_path)
            size_str = format_file_size(file_size)
            logger.info(f"ğŸ“¤ ä¸Šä¼ æ–‡ä»¶: {rel_path} ({size_str})")
            command = f'tess kubectl --cluster {cluster} -n {namespace} cp {file_path} {pod_name}:{os.path.join(remote_path, rel_path)}'
            
            # é‡è¯•æœºåˆ¶
            max_retries = 3
            start_time = time.time()
            for attempt in range(max_retries):
                try:
                    if debug:
                        logger.debug(f'æ‰§è¡Œå‘½ä»¤: {command}')
                    result = subprocess.run(command, shell=True, capture_output=True, text=True, check=True)
                    end_time = time.time()
                    sync_time = end_time - start_time
                    completed_files += 1
                    progress = (completed_files / total_files) * 100
                    if attempt > 0:
                        logger.success(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {rel_path} (è€—æ—¶: {sync_time:.2f}s) (é‡è¯• {attempt} æ¬¡åæˆåŠŸ) [{progress:.1f}%]")
                    else:
                        logger.success(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {rel_path} (è€—æ—¶: {sync_time:.2f}s) [{progress:.1f}%]")
                    return True
                except subprocess.CalledProcessError as e:
                    # è®°å½•é”™è¯¯è¾“å‡º
                    if e.stderr and e.stderr.strip():
                        logger.error(f"âŒ å‘½ä»¤æ‰§è¡Œé”™è¯¯: {e.stderr.strip()}")
                    
                    if attempt < max_retries - 1:
                        logger.warning(f"ğŸ”„ é‡è¯•æ–‡ä»¶ä¸Šä¼ : {rel_path} (ç¬¬ {attempt + 1} æ¬¡å°è¯•)")
                    else:
                        end_time = time.time()
                        sync_time = end_time - start_time
                        completed_files += 1
                        progress = (completed_files / total_files) * 100
                        logger.error(f"âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {rel_path} - é”™è¯¯ç : {e.returncode} ({max_retries} æ¬¡é‡è¯•åå¤±è´¥) (è€—æ—¶: {sync_time:.2f}s) [{progress:.1f}%]")
                        return False
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘ä¸Šä¼ 
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            logger.info(f"ğŸ“Š å¼€å§‹å¹¶å‘ä¸Šä¼ ï¼Œæœ€å¤§å¹¶å‘æ•°: {max_workers}")
            
            future_to_file = {executor.submit(upload_single_file, file_info): file_info for file_info in files_to_upload}
            
            successful_uploads = 0
            failed_uploads = 0
            
            for future in future_to_file:
                try:
                    if future.result():
                        successful_uploads += 1
                    else:
                        failed_uploads += 1
                except Exception as e:
                    file_path = future_to_file[future][0]
                    logger.error(f"âŒ æ–‡ä»¶ä¸Šä¼ å¼‚å¸¸: {file_path} - é”™è¯¯: {e}")
                    failed_uploads += 1
            
            logger.info(f"\nğŸ“Š ä¸Šä¼ å®Œæˆç»Ÿè®¡: âœ… {successful_uploads} æˆåŠŸ, âŒ {failed_uploads} å¤±è´¥")
    else:
        logger.info(f"\næ— æ–‡ä»¶éœ€è¦ä¸Šä¼ ")
    
    end_time = time.time()
    total_time = end_time - start_time
    logger.info(f"\nâ±ï¸  åˆå§‹åŒæ­¥å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f} ç§’")
    logger.info("=" * 60)
    logger.success("ğŸ‰ åˆå§‹åŒæ­¥å®Œæˆï¼å¼€å§‹æ–‡ä»¶å˜æ›´ç›‘å¬...")
    logger.info("=" * 60)

# ========== æ–‡ä»¶ç›‘å¬å¤„ç†å™¨ ==========

class FileChangeHandler(FileSystemEventHandler):
    """å¤„ç†æ–‡ä»¶å˜æ›´äº‹ä»¶çš„ç›‘å¬å™¨"""
    
    def __init__(self, local_path, namespace, pod_name, remote_path, cluster, executor, debug=False, show_concurrency=False, exclude_paths=None, debounce_seconds=1.0):
        self.local_path = local_path
        self.namespace = namespace
        self.pod_name = pod_name
        self.remote_path = remote_path
        self.cluster = cluster
        self.executor = executor
        self.debug = debug
        self.show_concurrency = show_concurrency
        self.exclude_paths = exclude_paths if exclude_paths is not None else []
        self.processing_files = {}  # è·Ÿè¸ªæ­£åœ¨å¤„ç†çš„æ–‡ä»¶
        self.debounce_timers = {}  # è·Ÿè¸ªæ¯ä¸ªæ–‡ä»¶çš„é˜²æŠ–å®šæ—¶å™¨
        self.debounce_seconds = debounce_seconds  # é˜²æŠ–å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        self.file_locks = {}  # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºé”ï¼Œé¿å…å¹¶å‘é—®é¢˜
        self.pending_uploads = {}  # è·Ÿè¸ªç­‰å¾…ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆå½“æ–‡ä»¶æ­£åœ¨ä¸Šä¼ æ—¶ï¼Œæ ‡è®°éœ€è¦å†æ¬¡ä¸Šä¼ ï¼‰
    
    def get_active_tasks_count(self):
        """è·å–å½“å‰æ´»è·ƒä»»åŠ¡æ•°"""
        return len([f for f in self.processing_files.values() if not f.done()])
    
    def get_active_files(self):
        """è·å–æ­£åœ¨å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨"""
        active_files = []
        for file_path, future in self.processing_files.items():
            if not future.done():
                rel_path = os.path.relpath(file_path, self.local_path)
                active_files.append(rel_path)
        return active_files
    
    def get_concurrency_info(self):
        """è·å–å¹¶å‘ä¿¡æ¯"""
        active_tasks = self.get_active_tasks_count()
        max_workers = self.executor._max_workers
        total_processing = len(self.processing_files)
        completed_tasks = total_processing - active_tasks
        
        return {
            'active': active_tasks,
            'max': max_workers,
            'available': max_workers - active_tasks,
            'total_processing': total_processing,
            'completed': completed_tasks
        }
    
    def print_concurrency_status(self):
        """æ‰“å°å¹¶å‘çŠ¶æ€"""
        concurrency_info = self.get_concurrency_info()
        active = concurrency_info['active']
        max_workers = concurrency_info['max']
        available = concurrency_info['available']
        
        # æ ¹æ®å¹¶å‘æ°´å¹³é€‰æ‹©ä¸åŒå›¾æ ‡
        if active == 0:
            icon = "ğŸŸ¢"
        elif active < max_workers * 0.5:
            icon = "ğŸŸ¡"
        elif active < max_workers * 0.8:
            icon = "ğŸŸ "
        else:
            icon = "ğŸ”´"
        
        usage_percent = (active / max_workers) * 100 if max_workers > 0 else 0
        
        if self.show_concurrency:
            completed = concurrency_info['completed']
            total_processing = concurrency_info['total_processing']
            logger.info(f"{icon} å¹¶å‘çŠ¶æ€: {active}/{max_workers} (å¯ç”¨: {available}, ä½¿ç”¨ç‡: {usage_percent:.1f}%, æ€»å¤„ç†: {total_processing}, å·²å®Œæˆ: {completed})")
            
            if active > 0:
                active_files = self.get_active_files()
                if active_files:
                    logger.info(f"   æ­£åœ¨å¤„ç†: {', '.join(active_files[:3])}{'...' if len(active_files) > 3 else ''}")
        else:
            completed = concurrency_info['completed']
            total_processing = concurrency_info['total_processing']
            logger.info(f"{icon} å¹¶å‘: {active}/{max_workers} (æ€»å¤„ç†: {total_processing}, å·²å®Œæˆ: {completed})")
    
    def on_modified(self, event):
        """æ–‡ä»¶ä¿®æ”¹äº‹ä»¶"""
        if event.is_directory:
            return
        
        # æ’é™¤éšè—æ–‡ä»¶
        if any(part.startswith('.') for part in event.src_path.split(os.sep)):
            return
        
        # æ’é™¤ä¸´æ—¶æ–‡ä»¶å’Œå¤‡ä»½æ–‡ä»¶
        file_name = os.path.basename(event.src_path)
        # å¸¸è§çš„ä¸´æ—¶æ–‡ä»¶åç¼€å’Œæ¨¡å¼
        temp_patterns = [
            file_name.endswith('~'),           # vim/emacs å¤‡ä»½æ–‡ä»¶
            file_name.endswith('.swp'),        # vim äº¤æ¢æ–‡ä»¶
            file_name.endswith('.swo'),        # vim äº¤æ¢æ–‡ä»¶
            file_name.endswith('.swn'),        # vim äº¤æ¢æ–‡ä»¶
            file_name.endswith('.tmp'),        # ä¸´æ—¶æ–‡ä»¶
            file_name.endswith('.bak'),        # å¤‡ä»½æ–‡ä»¶
            file_name.startswith('.#'),        # emacs é”æ–‡ä»¶
            file_name.endswith('#'),           # emacs è‡ªåŠ¨ä¿å­˜æ–‡ä»¶
            file_name.startswith('~'),         # ä¸´æ—¶æ–‡ä»¶
            '.tmp.' in file_name,              # ä¸´æ—¶æ–‡ä»¶
            file_name.endswith('.temp'),       # ä¸´æ—¶æ–‡ä»¶
        ]
        if any(temp_patterns):
            if self.debug:
                logger.debug(f'å¿½ç•¥ä¸´æ—¶æ–‡ä»¶: {file_name}')
            return
        
        # æ£€æŸ¥æ˜¯å¦åº”æ’é™¤
        rel_path = os.path.relpath(event.src_path, self.local_path)
        for exclude_path in self.exclude_paths:
            if rel_path == exclude_path or rel_path.startswith(exclude_path + os.sep) or rel_path.startswith(exclude_path + '/'):
                if self.debug:
                    logger.debug(f'å¿½ç•¥ä¿®æ”¹æ–‡ä»¶: {rel_path} (åŒ¹é…æ’é™¤æ¨¡å¼: {exclude_path})')
                return
        
        self.upload_file(event.src_path)
    
    def on_created(self, event):
        """æ–‡ä»¶åˆ›å»ºäº‹ä»¶"""
        if event.is_directory:
            return
        
        # æ’é™¤éšè—æ–‡ä»¶
        if any(part.startswith('.') for part in event.src_path.split(os.sep)):
            return
        
        # æ’é™¤ä¸´æ—¶æ–‡ä»¶å’Œå¤‡ä»½æ–‡ä»¶
        file_name = os.path.basename(event.src_path)
        # å¸¸è§çš„ä¸´æ—¶æ–‡ä»¶åç¼€å’Œæ¨¡å¼
        temp_patterns = [
            file_name.endswith('~'),           # vim/emacs å¤‡ä»½æ–‡ä»¶
            file_name.endswith('.swp'),        # vim äº¤æ¢æ–‡ä»¶
            file_name.endswith('.swo'),        # vim äº¤æ¢æ–‡ä»¶
            file_name.endswith('.swn'),        # vim äº¤æ¢æ–‡ä»¶
            file_name.endswith('.tmp'),        # ä¸´æ—¶æ–‡ä»¶
            file_name.endswith('.bak'),        # å¤‡ä»½æ–‡ä»¶
            file_name.startswith('.#'),        # emacs é”æ–‡ä»¶
            file_name.endswith('#'),           # emacs è‡ªåŠ¨ä¿å­˜æ–‡ä»¶
            file_name.startswith('~'),         # ä¸´æ—¶æ–‡ä»¶
            '.tmp.' in file_name,              # ä¸´æ—¶æ–‡ä»¶
            file_name.endswith('.temp'),       # ä¸´æ—¶æ–‡ä»¶
        ]
        if any(temp_patterns):
            if self.debug:
                logger.debug(f'å¿½ç•¥ä¸´æ—¶æ–‡ä»¶: {file_name}')
            return
        
        # æ£€æŸ¥æ˜¯å¦åº”æ’é™¤
        rel_path = os.path.relpath(event.src_path, self.local_path)
        for exclude_path in self.exclude_paths:
            if rel_path == exclude_path or rel_path.startswith(exclude_path + os.sep) or rel_path.startswith(exclude_path + '/'):
                if self.debug:
                    logger.debug(f'å¿½ç•¥åˆ›å»ºæ–‡ä»¶: {rel_path} (åŒ¹é…æ’é™¤æ¨¡å¼: {exclude_path})')
                return
        
        self.upload_file(event.src_path)
    
    def upload_file(self, file_path):
        """ä¸Šä¼ æ–‡ä»¶åˆ° Podï¼ˆå¸¦é˜²æŠ–åŠ¨ï¼‰"""
        rel_path = os.path.relpath(file_path, self.local_path)
        
        # å¦‚æœè¯¥æ–‡ä»¶å·²æœ‰é˜²æŠ–å®šæ—¶å™¨åœ¨è¿è¡Œï¼Œå–æ¶ˆå®ƒ
        if file_path in self.debounce_timers:
            old_timer = self.debounce_timers[file_path]
            if old_timer.is_alive():
                if self.debug:
                    logger.debug(f"å–æ¶ˆæ—§çš„é˜²æŠ–å®šæ—¶å™¨: {rel_path}")
                old_timer.cancel()
            del self.debounce_timers[file_path]
        
        # å¦‚æœè¯¥æ–‡ä»¶æ­£åœ¨ä¸Šä¼ ï¼Œä¸è¦å–æ¶ˆå®ƒï¼Œè€Œæ˜¯è®¾ç½®å¾…ä¸Šä¼ æ ‡è®°
        if file_path in self.processing_files:
            old_future = self.processing_files[file_path]
            if not old_future.done():
                # è®¾ç½®å¾…ä¸Šä¼ æ ‡è®°ï¼Œè®©ä¸Šä¼ å®Œæˆåé‡æ–°è§¦å‘
                self.pending_uploads[file_path] = True
                if self.debug:
                    logger.debug(f"æ–‡ä»¶æ­£åœ¨ä¸Šä¼ ä¸­ï¼Œè®¾ç½®å¾…ä¸Šä¼ æ ‡è®°: {rel_path}")
                # ä¸åˆ›å»ºæ–°çš„é˜²æŠ–å®šæ—¶å™¨ï¼Œè®©å½“å‰ä¸Šä¼ å®Œæˆåè‡ªåŠ¨å¤„ç†
                return
        
        logger.info(f"ğŸ” æ£€æµ‹åˆ°æ–‡ä»¶å˜æ›´: {rel_path} (å°†åœ¨ {self.debounce_seconds}s åä¸Šä¼ )")
        
        # åˆ›å»ºæ–°çš„é˜²æŠ–å®šæ—¶å™¨
        timer = threading.Timer(self.debounce_seconds, self._debounced_upload, args=[file_path])
        self.debounce_timers[file_path] = timer
        timer.start()
    
    def _debounced_upload(self, file_path):
        """é˜²æŠ–åå®é™…æ‰§è¡Œä¸Šä¼ """
        rel_path = os.path.relpath(file_path, self.local_path)
        
        # ä»é˜²æŠ–å®šæ—¶å™¨å­—å…¸ä¸­ç§»é™¤
        if file_path in self.debounce_timers:
            del self.debounce_timers[file_path]
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»åœ¨ä¸Šä¼ ä¸­
        if file_path in self.processing_files:
            old_future = self.processing_files[file_path]
            if not old_future.done():
                # æ–‡ä»¶æ­£åœ¨ä¸Šä¼ ä¸­ï¼Œæ ‡è®°éœ€è¦åœ¨å½“å‰ä¸Šä¼ å®Œæˆåå†æ¬¡ä¸Šä¼ 
                self.pending_uploads[file_path] = True
                if self.debug:
                    logger.debug(f"æ–‡ä»¶æ­£åœ¨ä¸Šä¼ ä¸­ï¼Œæ ‡è®°ä¸ºå¾…ä¸Šä¼ : {rel_path}")
                return
        
        if self.debug:
            logger.debug(f"é˜²æŠ–å®Œæˆï¼Œå¼€å§‹ä¸Šä¼ : {rel_path}")
        
        # æäº¤ä¸Šä¼ ä»»åŠ¡
        future = self.executor.submit(self._upload_file, file_path)
        self.processing_files[file_path] = future
    
    def _upload_file(self, file_path):
        """å®é™…ä¸Šä¼ æ–‡ä»¶çš„å†…éƒ¨æ–¹æ³•"""
        start_time = time.time()
        try:
            rel_path = os.path.relpath(file_path, self.local_path)
            file_size = os.path.getsize(file_path)
            size_str = format_file_size(file_size)
            
            # ç¡®ä¿è¿œç¨‹ç›®å½•å­˜åœ¨
            remote_dir = os.path.dirname(os.path.join(self.remote_path, rel_path))
            mkdir_command = f'tess kubectl --cluster {self.cluster} -n {self.namespace} exec {self.pod_name} -- mkdir -p {remote_dir}'
            if self.debug:
                logger.debug(f'æ‰§è¡Œå‘½ä»¤: {mkdir_command}')
            result = subprocess.run(mkdir_command, shell=True, capture_output=True, text=True)
            if result.returncode != 0 and result.stderr:
                logger.error(f"âŒ åˆ›å»ºè¿œç¨‹ç›®å½•å¤±è´¥: {result.stderr.strip()}")
            
            # ä¸Šä¼ æ–‡ä»¶
            command = f'tess kubectl --cluster {self.cluster} -n {self.namespace} cp {file_path} {self.pod_name}:{os.path.join(self.remote_path, rel_path)}'
            
            # é‡è¯•æœºåˆ¶
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    if self.debug:
                        logger.debug(f'æ‰§è¡Œå‘½ä»¤: {command}')
                    result = subprocess.run(command, shell=True, capture_output=True, text=True, check=True)
                    end_time = time.time()
                    sync_time = end_time - start_time
                    if attempt > 0:
                        logger.success(f"âœ… æ–‡ä»¶åŒæ­¥æˆåŠŸ: {rel_path} (è€—æ—¶: {sync_time:.2f}s) (é‡è¯• {attempt} æ¬¡åæˆåŠŸ) [å®æ—¶åŒæ­¥]")
                    else:
                        logger.success(f"âœ… æ–‡ä»¶åŒæ­¥æˆåŠŸ: {rel_path} (è€—æ—¶: {sync_time:.2f}s) [å®æ—¶åŒæ­¥]")
                    return
                except subprocess.CalledProcessError as e:
                    # è®°å½•é”™è¯¯è¾“å‡º
                    if e.stderr and e.stderr.strip():
                        logger.error(f"âŒ å‘½ä»¤æ‰§è¡Œé”™è¯¯: {e.stderr.strip()}")
                    
                    if attempt < max_retries - 1:
                        logger.warning(f"ğŸ”„ é‡è¯•æ–‡ä»¶åŒæ­¥: {rel_path} (ç¬¬ {attempt + 1} æ¬¡å°è¯•)")
                    else:
                        end_time = time.time()
                        sync_time = end_time - start_time
                        logger.error(f"âŒ æ–‡ä»¶åŒæ­¥å¤±è´¥: {rel_path} - é”™è¯¯ç : {e.returncode} ({max_retries} æ¬¡é‡è¯•åå¤±è´¥) (è€—æ—¶: {sync_time:.2f}s)")
        finally:
            # æ— è®ºæˆåŠŸè¿˜æ˜¯å¤±è´¥ï¼Œéƒ½ä»å¤„ç†åˆ—è¡¨ä¸­ç§»é™¤
            if file_path in self.processing_files:
                del self.processing_files[file_path]
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¾…ä¸Šä¼ çš„æ ‡è®°
            if file_path in self.pending_uploads:
                del self.pending_uploads[file_path]
                rel_path = os.path.relpath(file_path, self.local_path)
                if self.debug:
                    logger.debug(f"æ£€æµ‹åˆ°å¾…ä¸Šä¼ æ ‡è®°ï¼Œé‡æ–°å¯åŠ¨é˜²æŠ–: {rel_path}")
                # ä¸ç«‹å³ä¸Šä¼ ï¼Œè€Œæ˜¯é‡æ–°è§¦å‘é˜²æŠ–ï¼Œé¿å…æ–‡ä»¶è¿˜åœ¨ç¼–è¾‘ä¸­
                # è¿™æ ·å¯ä»¥åˆå¹¶ä¸Šä¼ æœŸé—´çš„å¤šæ¬¡ä¿®æ”¹
                timer = threading.Timer(self.debounce_seconds, self._debounced_upload, args=[file_path])
                self.debounce_timers[file_path] = timer
                timer.start()
                logger.info(f"ğŸ”„ æ£€æµ‹åˆ°æ–°å˜æ›´ï¼Œå°†åœ¨ {self.debounce_seconds}s åé‡æ–°ä¸Šä¼ : {rel_path}")

# ========== åˆå§‹åŒ–é…ç½® ==========

def init_config(project_name, local_path):
    """åˆå§‹åŒ–é…ç½®æ–‡ä»¶"""
    config_path = get_config_path(project_name)
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    if config_path.exists():
        # æ£€æŸ¥æ˜¯å¦ç¼ºå°‘ local_path å­—æ®µï¼Œå¦‚æœç¼ºå°‘åˆ™è¡¥å……
        config = load_config(project_name)
        if 'local_path' not in config or not config.get('local_path'):
            config['local_path'] = local_path
            save_config(project_name, config)
            logger.info("=" * 60)
            logger.success("âœ… é…ç½®æ–‡ä»¶å·²æ›´æ–°ï¼ˆæ·»åŠ  local_pathï¼‰")
            logger.info("=" * 60)
            logger.info(f"ğŸ“ é…ç½®æ–‡ä»¶ä½ç½®: {config_path}")
            logger.info(f"ğŸ“ local_path: {local_path}")
            logger.info("=" * 60)
            logger.info("\nâœ¨ ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¼€å§‹åŒæ­¥ï¼š")
            logger.info(f"   python3 {sys.argv[0]} --project {project_name}")
            logger.info("=" * 60)
        else:
            logger.info("=" * 60)
            logger.info("âš ï¸  é…ç½®æ–‡ä»¶å·²å­˜åœ¨")
            logger.info("=" * 60)
            logger.info(f"ğŸ“ é…ç½®æ–‡ä»¶ä½ç½®: {config_path}")
            logger.info("=" * 60)
            logger.info("\nğŸ“ è¯·ç›´æ¥ç¼–è¾‘é…ç½®æ–‡ä»¶ï¼š")
            logger.info(f"   vim {config_path}")
            logger.info("\nâœ¨ ç¼–è¾‘å®Œæˆåï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¼€å§‹åŒæ­¥ï¼š")
            logger.info(f"   python3 {sys.argv[0]} --project {project_name}")
            logger.info("=" * 60)
        return
    
    # åˆ›å»ºç¤ºä¾‹é…ç½®
    example_config = {
        "cluster": "908",
        "namespace": "your-namespace",
        "pod_label": "app=your-app",
        "remote_path": "/path/in/pod",
        "local_path": local_path,
        "compress_threshold": 50,
        "max_workers": 10,
        "debug": False,
        "show_concurrency": False,
        "no_watch": False,
        "skip_verify": False,
        "debounce_seconds": 1.0,
        "exclude_paths": [
            "ç¤ºä¾‹: node_modules",
            "ç¤ºä¾‹: *.log",
            "ç¤ºä¾‹: dist/build"
        ]
    }
    
    # ä¿å­˜é…ç½®
    save_config(project_name, example_config)
    
    logger.info("=" * 60)
    logger.success("âœ… é…ç½®æ–‡ä»¶å·²åˆ›å»º")
    logger.info("=" * 60)
    logger.info(f"ğŸ“ é…ç½®æ–‡ä»¶ä½ç½®: {config_path}")
    logger.info("=" * 60)
    logger.info("ğŸ“‹ å½“å‰é…ç½®å†…å®¹ï¼ˆç¤ºä¾‹ï¼‰:")
    logger.info("=" * 60)
    logger.info(json.dumps(example_config, indent=4, ensure_ascii=False))
    logger.info("=" * 60)
    logger.info("\nğŸ“ è¯·ç¼–è¾‘é…ç½®æ–‡ä»¶ï¼Œå¡«å…¥æ­£ç¡®çš„å‚æ•°å€¼ï¼š")
    logger.info(f"   vim {config_path}")
    logger.info("\nâœ¨ ç¼–è¾‘å®Œæˆåï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¼€å§‹åŒæ­¥ï¼š")
    logger.info(f"   python3 {sys.argv[0]} --project {project_name}")
    logger.info("=" * 60)

# ========== åˆ—å‡ºæ‰€æœ‰é¡¹ç›® ==========

def list_projects():
    """åˆ—å‡ºæ‰€æœ‰å·²é…ç½®çš„é¡¹ç›®"""
    home = Path.home()
    sync2pod_dir = home / '.sync2pod'
    
    if not sync2pod_dir.exists():
        logger.info("=" * 60)
        logger.info("ğŸ“‹ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•é¡¹ç›®é…ç½®")
        logger.info("=" * 60)
        logger.info("\nè¯·å…ˆåˆå§‹åŒ–é¡¹ç›®é…ç½®ï¼š")
        logger.info("  python3 sync_local_to_pod_optimized.py --init-config --local-path <æœ¬åœ°è·¯å¾„>")
        logger.info("=" * 60)
        return
    
    # æ”¶é›†æ‰€æœ‰é¡¹ç›®
    projects = []
    for project_dir in sync2pod_dir.iterdir():
        if project_dir.is_dir():
            config_file = project_dir / '.sync_config.json'
            if config_file.exists():
                try:
                    with open(config_file, 'r') as f:
                        config = json.load(f)
                    projects.append({
                        'name': project_dir.name,
                        'local_path': config.get('local_path', 'N/A'),
                        'remote_path': config.get('remote_path', 'N/A'),
                        'cluster': config.get('cluster', 'N/A'),
                        'namespace': config.get('namespace', 'N/A')
                    })
                except:
                    pass
    
    if not projects:
        logger.info("=" * 60)
        logger.info("ğŸ“‹ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„é¡¹ç›®é…ç½®")
        logger.info("=" * 60)
        return
    
    logger.info("=" * 60)
    logger.info(f"ğŸ“‹ å·²é…ç½®çš„é¡¹ç›® (å…± {len(projects)} ä¸ª)")
    logger.info("=" * 60)
    for i, proj in enumerate(projects, 1):
        logger.info(f"\n{i}. é¡¹ç›®å: {proj['name']}")
        logger.info(f"   æœ¬åœ°è·¯å¾„: {proj['local_path']}")
        logger.info(f"   è¿œç¨‹è·¯å¾„: {proj['remote_path']}")
        logger.info(f"   é›†ç¾¤: {proj['cluster']}")
        logger.info(f"   å‘½åç©ºé—´: {proj['namespace']}")
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ’¡ ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¼€å§‹åŒæ­¥ï¼š")
    logger.info(f"   python3 {sys.argv[0]} --project <é¡¹ç›®å>")
    logger.info("=" * 60)

# ========== ä¸»æµç¨‹ ==========

def main():
    parser = argparse.ArgumentParser(
        description='ä¼˜åŒ–ç‰ˆæœ¬åœ°ç›®å½•åŒæ­¥åˆ°K8s Podå·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:

  1. åˆå§‹åŒ–é…ç½®:
     python3 sync_local_to_pod_optimized.py --init-config --local-path /Users/xchen17/workspace/heketi
  
  2. æŸ¥çœ‹æ‰€æœ‰é¡¹ç›®:
     python3 sync_local_to_pod_optimized.py --list-projects
  
  3. å¼€å§‹åŒæ­¥:
     python3 sync_local_to_pod_optimized.py --project heketi
  
  4. å¼ºåˆ¶å…¨é‡åŒæ­¥:
     python3 sync_local_to_pod_optimized.py --project heketi --force
        """
    )
    
    # æ¨¡å¼é€‰æ‹©å‚æ•°
    parser.add_argument('--init-config', action='store_true', help='åˆå§‹åŒ–é…ç½®æ–‡ä»¶ï¼ˆéœ€é…åˆ --local-pathï¼‰')
    parser.add_argument('--list-projects', action='store_true', help='åˆ—å‡ºæ‰€æœ‰å·²é…ç½®çš„é¡¹ç›®')
    parser.add_argument('--project', help='é¡¹ç›®åç§°ï¼ˆç”¨äºåŒæ­¥ï¼‰')
    
    # åˆå§‹åŒ–æ‰€éœ€å‚æ•°
    parser.add_argument('--local-path', help='æœ¬åœ°ç›®å½•è·¯å¾„ï¼ˆä»…ç”¨äºåˆå§‹åŒ–ï¼‰')
    
    # åŒæ­¥å‚æ•°
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶å…¨é‡åŒæ­¥ï¼ˆä»…å‘½ä»¤è¡Œä½¿ç”¨ï¼Œä¸ä¿å­˜åˆ°é…ç½®æ–‡ä»¶ï¼‰')
    parser.add_argument('--skip-verify', action='store_true', help='è·³è¿‡åŒæ­¥å‰çš„é…ç½®ç¡®è®¤ï¼ˆå¯åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®ï¼Œé»˜è®¤ä¸º falseï¼‰')
    
    args = parser.parse_args()
    
    # æ¨¡å¼1ï¼šåˆ—å‡ºæ‰€æœ‰é¡¹ç›®
    if args.list_projects:
        list_projects()
        return
    
    # æ¨¡å¼2ï¼šåˆå§‹åŒ–é…ç½®
    if args.init_config:
        if not args.local_path:
            logger.error("âŒ é”™è¯¯: --init-config éœ€è¦é…åˆ --local-path ä½¿ç”¨")
            sys.exit(1)
        
        local_path = os.path.abspath(args.local_path)
        if not os.path.exists(local_path):
            logger.error(f"âŒ é”™è¯¯: æœ¬åœ°è·¯å¾„ä¸å­˜åœ¨: {local_path}")
            sys.exit(1)
        
        # ä»æœ¬åœ°è·¯å¾„æ¨å¯¼é¡¹ç›®å
        project_name = os.path.basename(os.path.normpath(local_path))
        init_config(project_name, local_path)
        return
    
    # æ¨¡å¼3ï¼šåŒæ­¥ - ä»…æ”¯æŒ --project
    if not args.project:
        logger.error("âŒ é”™è¯¯: è¯·æŒ‡å®šæ“ä½œæ¨¡å¼")
        logger.info("\nä½¿ç”¨è¯´æ˜:")
        logger.info("  åˆå§‹åŒ–:   python3 sync_local_to_pod_optimized.py --init-config --local-path <æœ¬åœ°è·¯å¾„>")
        logger.info("  æŸ¥çœ‹é¡¹ç›®: python3 sync_local_to_pod_optimized.py --list-projects")
        logger.info("  åŒæ­¥:     python3 sync_local_to_pod_optimized.py --project <é¡¹ç›®å>")
        sys.exit(1)
    
    project_name = args.project
    
    # åŠ è½½é…ç½®æ–‡ä»¶
    config_path = get_config_path(project_name)
    if not config_path.exists():
        logger.error(f"âŒ é”™è¯¯: é¡¹ç›® '{project_name}' çš„é…ç½®æ–‡ä»¶ä¸å­˜åœ¨")
        logger.info(f"\nè¯·å…ˆåˆå§‹åŒ–é…ç½®æˆ–æŸ¥çœ‹å¯ç”¨é¡¹ç›®:")
        logger.info(f"  åˆå§‹åŒ–: python3 {sys.argv[0]} --init-config --local-path <æœ¬åœ°è·¯å¾„>")
        logger.info(f"  æŸ¥çœ‹:   python3 {sys.argv[0]} --list-projects")
        sys.exit(1)
    
    config = load_config(project_name)
    
    # force å‚æ•°ä»…ä»å‘½ä»¤è¡Œè¯»å–ï¼Œä¸æŒä¹…åŒ–
    # skip_verify å¯ä»å‘½ä»¤è¡Œæˆ–é…ç½®æ–‡ä»¶è¯»å–ï¼Œå‘½ä»¤è¡Œä¼˜å…ˆ
    force_full_sync = args.force
    skip_verify = args.skip_verify if args.skip_verify else config.get('skip_verify', False)
    
    # éªŒè¯å¿…éœ€å‚æ•°
    required_fields = ['cluster', 'namespace', 'pod_label', 'remote_path', 'local_path']
    missing_fields = [field for field in required_fields if not config.get(field)]
    
    if missing_fields:
        logger.error(f"âŒ é”™è¯¯: é…ç½®æ–‡ä»¶ç¼ºå°‘å¿…éœ€å­—æ®µ: {', '.join(missing_fields)}")
        logger.info(f"\nè¯·ç¼–è¾‘é…ç½®æ–‡ä»¶: {config_path}")
        sys.exit(1)
    
    # è·å–å‚æ•°
    cluster = config['cluster']
    namespace = config['namespace']
    pod_label = config['pod_label']
    remote_path = config['remote_path']
    local_path = config['local_path']
    exclude_paths = config.get('exclude_paths', [])
    debug = config.get('debug', False)
    max_workers = config.get('max_workers', 10)
    show_concurrency = config.get('show_concurrency', False)
    no_watch = config.get('no_watch', False)
    debounce_seconds = config.get('debounce_seconds', 1.0)  # é»˜è®¤1ç§’é˜²æŠ–
    
    # å¦‚æœéœ€è¦éªŒè¯ï¼Œæ˜¾ç¤ºé‡è¦é…ç½®ä¿¡æ¯å¹¶ç­‰å¾…ç¡®è®¤
    if not skip_verify:
        logger.info("=" * 60)
        logger.info("âš ï¸  åŒæ­¥å‰é…ç½®ç¡®è®¤")
        logger.info("=" * 60)
        logger.info(f"é›†ç¾¤ (cluster):     {cluster}")
        logger.info(f"å‘½åç©ºé—´ (namespace): {namespace}")
        logger.info(f"Podæ ‡ç­¾ (pod_label): {pod_label}")
        logger.info(f"è¿œç¨‹è·¯å¾„ (remote_path): {remote_path}")
        logger.info(f"æœ¬åœ°è·¯å¾„ (local_path):  {local_path}")
        logger.info("=" * 60)
        logger.info("âš ï¸  è¯·ä»”ç»†æ ¸å¯¹ä»¥ä¸Šé…ç½®ï¼Œç¡®è®¤æ— è¯¯åæŒ‰å›è½¦ç»§ç»­...")
        logger.info("   (å¦‚éœ€è·³è¿‡æ­¤ç¡®è®¤ï¼Œå¯åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½® skip_verify: true")
        logger.info("    æˆ–ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•° --skip-verify)")
        logger.info("=" * 60)
        try:
            input()
        except KeyboardInterrupt:
            logger.error("\n\nâŒ ç”¨æˆ·å–æ¶ˆåŒæ­¥")
            sys.exit(0)
        logger.success("âœ… ç¡®è®¤å®Œæˆï¼Œå¼€å§‹åŒæ­¥...\n")
    
    # éªŒè¯æœ¬åœ°è·¯å¾„
    if not os.path.exists(local_path):
        logger.error(f"âŒ é”™è¯¯: æœ¬åœ°è·¯å¾„ä¸å­˜åœ¨: {local_path}")
        logger.info(f"\nè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„ local_path: {config_path}")
        sys.exit(1)
    
    if not (cluster and namespace and pod_label and remote_path):
        logger.error('cluster/namespace/pod_label/remote_path å¿…å¡«')
        sys.exit(1)
    
    # é…ç½®loggerï¼ˆæ ¹æ®debugæ¨¡å¼ï¼‰
    configure_logger(debug)
    
    # é€‰æ‹© pod
    pod_name = select_running_pod_by_label(cluster, namespace, pod_label)
    if debug:
        logger.debug(f'é€‰æ‹©åˆ° pod: {pod_name}')
    
    # åˆ¤æ–­åŒæ­¥æ–¹å¼
    if force_full_sync:
        # å¼ºåˆ¶å…¨é‡åŒæ­¥ï¼šç›´æ¥å‹ç¼©ä¸Šä¼ ï¼Œä¸åš MD5 å¯¹æ¯”
        force_sync_start = time.time()
        
        file_count = count_files(local_path, exclude_paths)
        if debug:
            logger.debug(f'æœ¬åœ°æ–‡ä»¶æ•°: {file_count}')
        logger.info(f'ğŸ—œï¸  å¼ºåˆ¶å…¨é‡åŒæ­¥æ¨¡å¼ï¼Œä½¿ç”¨å‹ç¼©æ‰“åŒ…ä¸Šä¼ ...')
        
        with tempfile.TemporaryDirectory() as tmpdir:
            tar_path = os.path.join(tmpdir, 'sync_upload.tar.gz')
            
            # 1. å‹ç¼©æ–‡ä»¶
            logger.info('ğŸ“¦ æ­£åœ¨å‹ç¼©æœ¬åœ°ç›®å½•...')
            compress_start = time.time()
            compress_dir(local_path, tar_path, exclude_paths, debug)
            compress_end = time.time()
            compress_time = compress_end - compress_start
            
            compressed_size = os.path.getsize(tar_path)
            logger.success(f'âœ… å‹ç¼©å®Œæˆ: {format_file_size(compressed_size)} (è€—æ—¶: {compress_time:.2f}s)')
            
            # è®¡ç®— remote_path çš„çˆ¶ç›®å½•
            remote_parent = os.path.dirname(remote_path)
            remote_tar_path = os.path.join(remote_parent, 'sync_upload.tar.gz')
            
            # ä¼˜åŒ–ï¼šåˆå¹¶å¤šä¸ª kubectl exec å‘½ä»¤å‡å°‘ IO
            cmd_cp = f'tess kubectl --cluster {cluster} -n {namespace} cp {tar_path} {pod_name}:{remote_tar_path}'
            # åˆå¹¶æ¸…ç©ºã€è§£å‹ã€åˆ é™¤ä¸ºä¸€æ¬¡ kubectl exec è°ƒç”¨
            cmd_extract = f'tess kubectl --cluster {cluster} -n {namespace} exec {pod_name} -- bash -c "rm -rf {remote_path}/* && tar -xzf {remote_tar_path} -C {remote_path} && rm {remote_tar_path}"'
            
            if debug:
                logger.debug(f'ä¸Šä¼ å‹ç¼©åŒ…å‘½ä»¤: {cmd_cp}')
                logger.debug(f'è§£å‹å¹¶æ¸…ç†å‘½ä»¤: {cmd_extract}')
            
            # 2. ä¸Šä¼ å‹ç¼©åŒ…
            logger.info('ğŸ“¤ ä¸Šä¼ å‹ç¼©åŒ…...')
            upload_start = time.time()
            os.system(cmd_cp)
            upload_end = time.time()
            upload_time = upload_end - upload_start
            logger.success(f'âœ… ä¸Šä¼ å®Œæˆ (è€—æ—¶: {upload_time:.2f}s)')
            
            # 3. è¿œç«¯è§£å‹
            logger.info('ğŸ“¦ è§£å‹å¹¶æ¸…ç† (æ¸…ç©º -> è§£å‹ -> åˆ é™¤ä¸´æ—¶æ–‡ä»¶)...')
            extract_start = time.time()
            os.system(cmd_extract)
            extract_end = time.time()
            extract_time = extract_end - extract_start
            logger.success(f'âœ… è§£å‹å®Œæˆ (è€—æ—¶: {extract_time:.2f}s)')
            
        force_sync_end = time.time()
        total_time = force_sync_end - force_sync_start
        
        logger.info("\n" + "=" * 60)
        logger.info("â±ï¸  å¼ºåˆ¶å…¨é‡åŒæ­¥è€—æ—¶ç»Ÿè®¡")
        logger.info("=" * 60)
        logger.info(f"  1. å‹ç¼©æ–‡ä»¶:   {compress_time:.2f}s")
        logger.info(f"  2. ä¸Šä¼ æ–‡ä»¶:   {upload_time:.2f}s")
        logger.info(f"  3. è¿œç«¯è§£å‹:   {extract_time:.2f}s")
        logger.info(f"  æ€»è€—æ—¶:        {total_time:.2f}s")
        logger.info("=" * 60)
    else:
        # æ™ºèƒ½å¢é‡åŒæ­¥ï¼šæ€»æ˜¯è¿›è¡Œ MD5 å¯¹æ¯”ï¼Œæ ¹æ®å¾…ä¸Šä¼ æ–‡ä»¶æ•°é€‰æ‹©ä¸Šä¼ æ–¹å¼
        file_count = count_files(local_path, exclude_paths)
        if debug:
            logger.debug(f'æœ¬åœ°æ–‡ä»¶æ•°: {file_count}')
        logger.info(f'ğŸ“Š æœ¬åœ°æ–‡ä»¶æ•°: {file_count}ï¼Œå¼€å§‹ MD5 å¯¹æ¯”å¢é‡åŒæ­¥...')
        upload_initial_files(local_path, namespace, pod_name, remote_path, cluster, debug, max_workers, exclude_paths)
    
    # å¯åŠ¨æ–‡ä»¶ç›‘å¬ï¼ˆé™¤éé…ç½®æ–‡ä»¶ä¸­æŒ‡å®š no_watchï¼‰
    if not no_watch:
        logger.info(f"ğŸ‘€ å¯åŠ¨æ–‡ä»¶å˜æ›´ç›‘å¬... (æœ€å¤§å¹¶å‘æ•°: {max_workers}, é˜²æŠ–å»¶è¿Ÿ: {debounce_seconds}s)")
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            event_handler = FileChangeHandler(
                local_path, namespace, pod_name, remote_path, cluster, 
                executor, debug, show_concurrency, exclude_paths, debounce_seconds
            )
            observer = Observer()
            observer.schedule(event_handler, path=local_path, recursive=True)
            observer.start()
            
            try:
                while True:
                    # æ¯ 30 ç§’æ˜¾ç¤ºå¹¶å‘çŠ¶æ€ï¼ˆä»…å½“å¯ç”¨è¯¦ç»†å¹¶å‘ä¿¡æ¯æ—¶ï¼‰
                    time.sleep(30)
                    if show_concurrency:
                        concurrency_info = event_handler.get_concurrency_info()
                        if concurrency_info['active'] > 0:
                            event_handler.print_concurrency_status()
            except KeyboardInterrupt:
                logger.info("\nâ¹ï¸  åœæ­¢æ–‡ä»¶ç›‘å¬...")
                observer.stop()
            observer.join()
    else:
        logger.success("âœ… åŒæ­¥å®Œæˆï¼ˆæ–‡ä»¶ç›‘å¬å·²ç¦ç”¨ï¼‰")

if __name__ == '__main__':
    # å…ˆé…ç½®åŸºæœ¬çš„loggerï¼ˆåˆå§‹åŒ–å’Œåˆ—è¡¨é¡¹ç›®æ¨¡å¼ï¼‰
    configure_logger(debug=False)
    main()
